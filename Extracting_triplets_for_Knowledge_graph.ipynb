{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extracting triplets for Knowledge graph",
      "provenance": [],
      "collapsed_sections": [
        "yavI5YeILnmG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqqAzSHpbpMS"
      },
      "source": [
        "Future work: fine tune ner\n",
        "\n",
        "Idea based on https://www.analyticsvidhya.com/blog/2020/06/nlp-project-information-extraction/:\n",
        " 1. Subject (nsubj) - combine compound and Object (dobj) extraction\n",
        " 2. Adjective Noun\n",
        " 3. We look for tokens that have a Noun POS tag and have subject or object dependency\n",
        " 4. Then we look at the child nodes of these tokens and append it to the phrase only if it modifies the noun\n",
        " 5. Rule on Prepositions\n",
        " 6. We iterate over all the tokens looking for prepositions. For example, in this sentence\n",
        " 7. On encountering a preposition, we check if it has a headword that is a noun. For example, the word faith in this sentence\n",
        " 8. Then we look at the child tokens of the preposition token falling on its right side. For example, the word democracy\n",
        " 9. Append modifier attached to a noun\n",
        " 10. if it has subject and noun: run relation extraction\n",
        " 11. run relation extraction between entities extracted from NER and add them if its score is above a threshold (0.8 for now but this could change)!\n",
        " 12. TODO: group a nouns with conjunctions as a single entity???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZQ49Br1Bpdq"
      },
      "source": [
        "## Extract data about Model.SPR\n",
        "Webscrape data from Swanton Pacific Ranch wikipedia page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49OMMofEBnXw"
      },
      "source": [
        "# https://en.wikipedia.org/wiki/Swanton_Pacific_Ranch\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "spr_wiki = requests.get('https://en.wikipedia.org/wiki/Swanton_Pacific_Ranch')\n",
        "\n",
        "soup = BeautifulSoup(spr_wiki.text, 'html.parser')\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGl_-n4qPa19"
      },
      "source": [
        "for script in soup([\"style\"]):                   \n",
        "    script.decompose() "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8UNu1aADvI_"
      },
      "source": [
        "spr_wiki_text = ' '.join(p.text.strip() for p in soup.find_all(\"div\", {\"class\": \"mw-parser-output\"})[0].find_all('p'))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VG5POVC8GijR"
      },
      "source": [
        "# remove special characters\n",
        "spr_wiki_text = spr_wiki_text.encode('ascii', 'ignore').decode('ascii')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xPLrVK-Gslu"
      },
      "source": [
        "# remove in-line citation\n",
        "import re\n",
        "spr_wiki_text = re.sub(r'\\[\\d*\\]', '', spr_wiki_text)\n",
        "spr_wiki_text = re.sub(r'Full Report', '', spr_wiki_text)\n",
        "# remove parenthesis\n",
        "spr_wiki_text = re.sub(r'\\(.*?\\) ', '', spr_wiki_text)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yavI5YeILnmG"
      },
      "source": [
        "## Coreference Resolution (https://github.com/huggingface/neuralcoref)\n",
        "Install Coreference Resolution - had to downgrade Spacy to 2.1.0 because of neuralcoref's incompatibility with Spacy >= 2.1.8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATqbrMEWy55E",
        "outputId": "a08efe71-07b3-4c9c-9869-5e02438dd360"
      },
      "source": [
        "import spacy\n",
        "\n",
        "print(spacy.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "Q-9HtoVgyeRP",
        "outputId": "4a2c02ea-f68d-430d-d0d3-5d043e3bbbc2"
      },
      "source": [
        "!pip install spacy==2.1.0"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy==2.1.0\n",
            "  Downloading spacy-2.1.0-cp37-cp37m-manylinux1_x86_64.whl (27.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 27.7 MB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.0.6)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.6.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.0.5)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (0.9.6)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (0.2.4)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (7.0.8)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2.10)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.0) (4.62.3)\n",
            "Installing collected packages: spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.1.8\n",
            "    Uninstalling spacy-2.1.8:\n",
            "      Successfully uninstalled spacy-2.1.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 2.2.5 requires spacy>=2.2.2, but you have spacy 2.1.0 which is incompatible.\u001b[0m\n",
            "Successfully installed spacy-2.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "spacy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        },
        "id": "E0Ja0eB9zrQa",
        "outputId": "07de3a4c-0810-43d7-dd15-34099cf3ae43"
      },
      "source": [
        "!pip install neuralcoref"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neuralcoref\n",
            "  Downloading neuralcoref-4.0-cp37-cp37m-manylinux1_x86_64.whl (286 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 19.6 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20 kB 21.4 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 30 kB 24.1 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 51 kB 30.6 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 61 kB 31.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 71 kB 31.2 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 81 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 92 kB 27.3 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 102 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 112 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 122 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 133 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 143 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 153 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 163 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 174 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 184 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 194 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 204 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 215 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 225 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 235 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 245 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 256 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 266 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 276 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 286 kB 28.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (2.23.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.20.17-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 28.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (1.19.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.6)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.1)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (7.0.8)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.9.6)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.8.2)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.6.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.1.0,>=7.0.2->spacy>=2.1.0->neuralcoref) (4.62.3)\n",
            "Collecting botocore<1.24.0,>=1.23.17\n",
            "  Downloading botocore-1.23.17-py3-none-any.whl (8.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.4 MB 29.5 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 57.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.17->boto3->neuralcoref) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.17->boto3->neuralcoref) (1.15.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, neuralcoref\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.20.17 botocore-1.23.17 jmespath-0.10.0 neuralcoref-4.0 s3transfer-0.5.0 urllib3-1.25.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrorvOG9T6Yg"
      },
      "source": [
        "## Spacy\n",
        "Load spacy english model and add coreference resolution to its pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWaw6dSZ0L1_",
        "outputId": "b21c24ee-38bd-4a77-9df4-489f3e162528"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1 MB 26.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.1.0-py3-none-any.whl size=11074431 sha256=e78d0aa08d14d62853c3ccddd079511a3394f623ae362728d2f7bb153337af25\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-48tyiasq/wheels/59/4f/8c/0dbaab09a776d1fa3740e9465078bfd903cc22f3985382b496\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFOzRYu2LAKk",
        "outputId": "a2965172-76b2-4e0c-aad6-3879f1e88085"
      },
      "source": [
        "import spacy\n",
        "import neuralcoref\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "neuralcoref.add_to_pipe(nlp)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7fbd9a881cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3gTjHKaNqoe"
      },
      "source": [
        "coref_doc = nlp(spr_wiki_text)\n",
        "# print(coref_doc._.coref_clusters)\n",
        "coref_resolved_spr_wiki_text = coref_doc._.coref_resolved"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYGu_hQmfy65"
      },
      "source": [
        "## Visualize Spacy Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwhsgmsPbfXu",
        "outputId": "93149164-0237-4cef-9db3-8848a253a204"
      },
      "source": [
        "!pip install visualise_spacy_tree"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting visualise_spacy_tree\n",
            "  Downloading visualise_spacy_tree-0.0.6-py3-none-any.whl (5.0 kB)\n",
            "Collecting pydot==1.4.1\n",
            "  Downloading pydot-1.4.1-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot==1.4.1->visualise_spacy_tree) (3.0.6)\n",
            "Installing collected packages: pydot, visualise-spacy-tree\n",
            "  Attempting uninstall: pydot\n",
            "    Found existing installation: pydot 1.3.0\n",
            "    Uninstalling pydot-1.3.0:\n",
            "      Successfully uninstalled pydot-1.3.0\n",
            "Successfully installed pydot-1.4.1 visualise-spacy-tree-0.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChIqXwLncQDx"
      },
      "source": [
        "from spacy import displacy \n",
        "import visualise_spacy_tree\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# doc = nlp(sentences[0])\n",
        "# png = visualise_spacy_tree.create_png(doc)\n",
        "# display(Image(png))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upzD1InGnBWC"
      },
      "source": [
        "def draw_dependency_graph(doc):\n",
        "  displacy.render(doc, style='dep', jupyter=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76Dz_fYVwPqw"
      },
      "source": [
        "## Rules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkCqU1vuNvgb"
      },
      "source": [
        "def check_btw_ends(idx, compound_indices):\n",
        "  for compound_idx in compound_indices:\n",
        "      if idx > compound_idx[0] and idx < compound_idx[1]:\n",
        "        return True\n",
        "  return False\n",
        "\n",
        "def can_extend_compound(idx, compound_indices):\n",
        "  for i in range(len(compound_indices)):\n",
        "    compound_idx = compound_indices[i]\n",
        "    if idx == compound_idx[1]:\n",
        "      return i\n",
        "  return -1\n",
        "\n",
        "# compounds\n",
        "def get_compounds(doc):\n",
        "  compounds = []\n",
        "  compound_indices = []\n",
        "  for token in doc:\n",
        "    if token.dep_ == 'compound':\n",
        "      # if current token.i is between previously found start and end indices of a compound, skip\n",
        "      if not check_btw_ends(token.i, compound_indices):\n",
        "        # if current token.i == end index of a previsouly found compound, extend\n",
        "        idx_to_extend = can_extend_compound(token.i, compound_indices)\n",
        "        if idx_to_extend != -1:\n",
        "          compound_indices[idx_to_extend][1] = token.head.i\n",
        "        else:\n",
        "          compound_indices.append([token.i, token.head.i])\n",
        "\n",
        "  compounds = [doc[compound_idx[0]: compound_idx[1] + 1] for compound_idx in compound_indices]\n",
        "\n",
        "  return compounds, compound_indices"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Unagy614_VE"
      },
      "source": [
        "def combine_verbs_with_conj(doc):\n",
        "  combined_verbs = []\n",
        "  combined_verbs_indices = []\n",
        "  for token in doc:\n",
        "    if token.pos_ == 'CCONJ':\n",
        "      combined_verb = ''\n",
        "      left = doc[token.i - 1]\n",
        "      right = doc[token.i + 1]\n",
        "\n",
        "      if left.pos_ == 'VERB' and right.pos_ == 'VERB':\n",
        "        combined_verb += (left.text + ' ' + token.text + ' ' + right.text)\n",
        "        combined_verbs.append(combined_verb)\n",
        "        combined_verbs_indices.append((left.i, right.i))\n",
        "  return combined_verbs, combined_verbs_indices"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6CaW9xL4Ayo"
      },
      "source": [
        "# combine compounds and nouns with a modifier to a single token\n",
        "def update_tokenizer(doc):\n",
        "  compounds, compounds_indices = get_compounds(doc)\n",
        "  assert len(compounds) == len(compounds_indices)\n",
        "\n",
        "  with doc.retokenize() as retokenizer:\n",
        "    for i in range(len(compounds)):\n",
        "      compound = compounds[i]\n",
        "      retokenizer.merge(doc[compounds_indices[i][0]: compounds_indices[i][1] + 1], attrs={\"LEMMA\": compound.text.lower()})\n",
        "\n",
        "  mod_nouns, mod_nouns_indices = get_noun_mod(doc)\n",
        "  assert len(mod_nouns) == len(mod_nouns_indices)\n",
        "\n",
        "  with doc.retokenize() as retokenizer:\n",
        "    for i in range(len(mod_nouns)):\n",
        "      mod_noun = mod_nouns[i]\n",
        "      retokenizer.merge(doc[mod_nouns_indices[i][0]: mod_nouns_indices[i][-1] + 1], attrs={\"LEMMA\": mod_noun.lower()})\n",
        "\n",
        "  combined_verbs, combined_verbs_indices = combine_verbs_with_conj(doc)\n",
        "\n",
        "  assert len(combined_verbs) == len(combined_verbs_indices)\n",
        "\n",
        "  with doc.retokenize() as retokenizer:\n",
        "    for i in range(len(combined_verbs)):\n",
        "      combined_verb = combined_verbs[i]\n",
        "      retokenizer.merge(doc[combined_verbs_indices[i][0]: combined_verbs_indices[i][-1] + 1], attrs={\"LEMMA\": combined_verb.lower()})\n",
        "\n",
        "  return doc"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coawuhHDrD5C"
      },
      "source": [
        "def split_verb_w_conj(subject, root_verb, obj, preposition=''):\n",
        "  triples = []\n",
        "  if 'or' in root_verb:\n",
        "    verbs = root_verb.split('or')\n",
        "    for verb in verbs:\n",
        "      triples.append((subject, verb.strip() + ' ' + preposition, obj))\n",
        "  elif 'and' in root_verb:\n",
        "    verbs = root_verb.split('and')\n",
        "    for verb in verbs:\n",
        "      triples.append((subject, verb.strip() + ' ' + preposition, obj))\n",
        "  else:\n",
        "    triples.append((subject, root_verb.strip() + ' ' + preposition, obj))\n",
        "  return triples"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZejE5yCeYSwc"
      },
      "source": [
        "# function for rule 1: noun(subject), verb, noun(object)\n",
        "def rule_1(doc):\n",
        "        \n",
        "    sent = []\n",
        "    \n",
        "    for token in doc:\n",
        "        \n",
        "        # if the token is a verb\n",
        "        if (token.pos_=='VERB'):\n",
        "            \n",
        "            # phrase = []\n",
        "            subject = ''\n",
        "            verb = ''\n",
        "            # only extract noun or pronoun subjects\n",
        "            for sub_tok in token.lefts:\n",
        "                \n",
        "                if (sub_tok.dep_ in ['nsubj','nsubjpass']) and (sub_tok.pos_ in ['NOUN','PROPN','PRON']):\n",
        "                    \n",
        "                    # add subject to the phrase\n",
        "                    subject = sub_tok.text\n",
        "\n",
        "                    # save the root of the verb in phrase\n",
        "                    verb = token.text \n",
        "\n",
        "                    # check for noun or pronoun direct objects\n",
        "                    for sub_tok in token.rights:\n",
        "                        \n",
        "                        # save the object in the phrase\n",
        "                        if (sub_tok.dep_ in ['dobj']) and (sub_tok.pos_ in ['NOUN','PROPN']):\n",
        "                            obj = sub_tok.text\n",
        "                            phrase = split_verb_w_conj(subject, verb, obj)\n",
        "\n",
        "                            sent.append(tuple(phrase))\n",
        "            \n",
        "    return sent"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LAacLkSk9K9"
      },
      "source": [
        "# if a ROOT verb is a be verb\n",
        "def rule_2(doc):\n",
        "  verb = []\n",
        "  for token in doc:\n",
        "    if token.tag_.startswith('V') and token.pos_ == 'AUX' and token.dep_ == 'ROOT':\n",
        "      verb = [token, token.i]\n",
        "      break\n",
        "  if len(verb):\n",
        "    # make this triplets and add to the list\n",
        "    return [(doc[:verb[1]].text, verb[0].text, doc[verb[1] + 1:][0]), *rule_3(doc[verb[1] + 1:])]\n",
        "  else:\n",
        "    return ()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmDR8YCXuQo8"
      },
      "source": [
        "# get nouns with modifiers\n",
        "def get_noun_mod(doc):\n",
        "  nouns = []\n",
        "  nouns_indices = []\n",
        "  for token in doc:\n",
        "    if token.pos_ in ['NOUN', 'PROPN'] and token.dep_ in ['attr', 'pobj', 'dobj']:\n",
        "      modifier = ''\n",
        "      modifier_indices = []\n",
        "      for left in token.lefts:\n",
        "        if left.dep_ in ['det', 'nummod', 'nmod'] or left.pos_ == 'ADJ':\n",
        "          modifier += ' ' + left.text\n",
        "          modifier_indices.append(left.i)\n",
        "      if len(modifier):\n",
        "        modifier_indices.append(token.i)\n",
        "        nouns.append((modifier + ' ' + token.text).strip())\n",
        "        nouns_indices.append(modifier_indices)\n",
        "  return nouns, nouns_indices\n",
        "\n",
        "# rule 3 noun + preposition + noun\n",
        "def rule_3(doc):\n",
        "        \n",
        "    sent = []\n",
        "    \n",
        "    for token in doc:\n",
        "\n",
        "        # look for prepositions\n",
        "        if token.pos_=='ADP':\n",
        "\n",
        "            phrase = []\n",
        "            \n",
        "            # if its head word is a noun\n",
        "            if token.head.pos_=='NOUN':\n",
        "                \n",
        "                # append noun and preposition to phrase\n",
        "                phrase.append(token.head.text)\n",
        "\n",
        "                phrase.append(token.text)\n",
        "\n",
        "                # check the nodes to the right of the preposition\n",
        "                for right_tok in token.rights:\n",
        "                    # append if it is a noun or proper noun\n",
        "                    if (right_tok.pos_ in ['NOUN','PROPN']):\n",
        "                      phrase.append(right_tok.text)\n",
        "                \n",
        "                if len(phrase) > 2:\n",
        "                    sent.append(tuple(phrase))\n",
        "                \n",
        "    return sent "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQAYLUEw8gcS"
      },
      "source": [
        "# handle passive sentences\n",
        "def rule_4(doc):\n",
        "  all_triples = []\n",
        "  root_verb = ''\n",
        "  subject = ''\n",
        "  obj = ''\n",
        "  for token in doc:\n",
        "    # passive verb\n",
        "    if token.pos_ == 'AUX' and token.head.pos_ == 'VERB':\n",
        "      subject = doc[:token.i].text\n",
        "      root_verb = token.head.text\n",
        "    \n",
        "    if token.pos_ in ['PROPN', 'NOUN']:\n",
        "      if (token.dep_ == 'conj' and token.head.head.head.text == root_verb):\n",
        "        obj = token.text\n",
        "        triples = split_verb_w_conj(subject, root_verb, obj, token.head.head.text)\n",
        "        if len(triples):\n",
        "          all_triples.extend(triples)\n",
        "        obj = ''\n",
        "      elif (token.head.head.text == root_verb):\n",
        "        obj = token.text\n",
        "        triples = split_verb_w_conj(subject, root_verb, obj, token.head.text)\n",
        "        if len(triples):\n",
        "          all_triples.extend(triples)\n",
        "        obj = ''\n",
        "  return all_triples"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRsdREiuDFeT"
      },
      "source": [
        "# from NER\n",
        "def get_named_entities(doc):\n",
        "  entities = []\n",
        "  for ent in doc.ents:\n",
        "    entities.append((ent, ent.label_, (ent.start, ent.end)))\n",
        "  return entities"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpFggLYNdsDx"
      },
      "source": [
        "## Relation Extraction\n",
        "OpenNRE https://github.com/thunlp/OpenNRE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKnGmhCyfOT0"
      },
      "source": [
        "### Install OpenNRE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDmw1i4fdtkQ",
        "outputId": "f9955da5-79e4-49db-872b-0ac7b434029c"
      },
      "source": [
        "!git clone https://github.com/thunlp/OpenNRE.git --depth 1\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'OpenNRE'...\n",
            "remote: Enumerating objects: 68, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
            "remote: Total 68 (delta 21), reused 29 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (68/68), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_Ev_cTHd1dB",
        "outputId": "029ec00c-4a60-4126-b448-705b8c666e9f"
      },
      "source": [
        "%cd OpenNRE/"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/OpenNRE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5IkpyGkRdwuV",
        "outputId": "26f207d0-8f05-4620-cc67-c2a6a0775453"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.6.0\n",
            "  Downloading torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8 MB 17 kB/s \n",
            "\u001b[?25hCollecting transformers==3.4.0\n",
            "  Downloading transformers-3.4.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 48.7 MB/s \n",
            "\u001b[?25hCollecting pytest==5.3.2\n",
            "  Downloading pytest-5.3.2-py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 37.7 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.22.1\n",
            "  Downloading scikit_learn-0.22.1-cp37-cp37m-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 52.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.4.1)\n",
            "Collecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 41.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->-r requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->-r requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 54.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (21.3)\n",
            "Collecting tokenizers==0.9.2\n",
            "  Downloading tokenizers-0.9.2-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 43.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (2019.12.20)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (3.17.3)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 53.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (8.12.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (1.11.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (4.8.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (0.2.5)\n",
            "Collecting pluggy<1.0,>=0.12\n",
            "  Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (21.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.1->-r requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5->-r requirements.txt (line 6)) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest==5.3.2->-r requirements.txt (line 3)) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest==5.3.2->-r requirements.txt (line 3)) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.4.0->-r requirements.txt (line 2)) (3.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r requirements.txt (line 2)) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r requirements.txt (line 2)) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0->-r requirements.txt (line 2)) (7.1.2)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449923 sha256=a3a6259f50b41325e8f46a3dd6b07e501e86d26b75631ad6bcf67ea310615d58\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "Successfully built nltk\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, pluggy, transformers, torch, scikit-learn, pytest, nltk\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.1\n",
            "    Uninstalling scikit-learn-1.0.1:\n",
            "      Successfully uninstalled scikit-learn-1.0.1\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.6.0 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.22.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed nltk-3.4.5 pluggy-0.13.1 pytest-5.3.2 sacremoses-0.0.46 scikit-learn-0.22.1 sentencepiece-0.1.96 tokenizers-0.9.2 torch-1.6.0 transformers-3.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOLVjnbdd5YG",
        "outputId": "284f4723-f453-43b7-c733-26d784b42514"
      },
      "source": [
        "!python setup.py install"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating opennre.egg-info\n",
            "writing opennre.egg-info/PKG-INFO\n",
            "writing dependency_links to opennre.egg-info/dependency_links.txt\n",
            "writing top-level names to opennre.egg-info/top_level.txt\n",
            "writing manifest file 'opennre.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'opennre.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/opennre\n",
            "copying opennre/pretrain.py -> build/lib/opennre\n",
            "copying opennre/__init__.py -> build/lib/opennre\n",
            "creating build/lib/opennre/tokenization\n",
            "copying opennre/tokenization/bert_tokenizer.py -> build/lib/opennre/tokenization\n",
            "copying opennre/tokenization/basic_tokenizer.py -> build/lib/opennre/tokenization\n",
            "copying opennre/tokenization/__init__.py -> build/lib/opennre/tokenization\n",
            "copying opennre/tokenization/word_piece_tokenizer.py -> build/lib/opennre/tokenization\n",
            "copying opennre/tokenization/word_tokenizer.py -> build/lib/opennre/tokenization\n",
            "copying opennre/tokenization/utils.py -> build/lib/opennre/tokenization\n",
            "creating build/lib/opennre/framework\n",
            "copying opennre/framework/multi_label_sentence_re.py -> build/lib/opennre/framework\n",
            "copying opennre/framework/data_loader.py -> build/lib/opennre/framework\n",
            "copying opennre/framework/__init__.py -> build/lib/opennre/framework\n",
            "copying opennre/framework/sentence_re.py -> build/lib/opennre/framework\n",
            "copying opennre/framework/bag_re.py -> build/lib/opennre/framework\n",
            "copying opennre/framework/utils.py -> build/lib/opennre/framework\n",
            "creating build/lib/opennre/module\n",
            "copying opennre/module/__init__.py -> build/lib/opennre/module\n",
            "creating build/lib/opennre/model\n",
            "copying opennre/model/__init__.py -> build/lib/opennre/model\n",
            "copying opennre/model/bag_attention.py -> build/lib/opennre/model\n",
            "copying opennre/model/sigmoid_nn.py -> build/lib/opennre/model\n",
            "copying opennre/model/bag_one.py -> build/lib/opennre/model\n",
            "copying opennre/model/base_model.py -> build/lib/opennre/model\n",
            "copying opennre/model/bag_average.py -> build/lib/opennre/model\n",
            "copying opennre/model/softmax_nn.py -> build/lib/opennre/model\n",
            "creating build/lib/opennre/encoder\n",
            "copying opennre/encoder/pcnn_encoder.py -> build/lib/opennre/encoder\n",
            "copying opennre/encoder/cnn_encoder.py -> build/lib/opennre/encoder\n",
            "copying opennre/encoder/bert_encoder.py -> build/lib/opennre/encoder\n",
            "copying opennre/encoder/base_encoder.py -> build/lib/opennre/encoder\n",
            "copying opennre/encoder/__init__.py -> build/lib/opennre/encoder\n",
            "creating build/lib/opennre/module/nn\n",
            "copying opennre/module/nn/rnn.py -> build/lib/opennre/module/nn\n",
            "copying opennre/module/nn/cnn.py -> build/lib/opennre/module/nn\n",
            "copying opennre/module/nn/__init__.py -> build/lib/opennre/module/nn\n",
            "copying opennre/module/nn/lstm.py -> build/lib/opennre/module/nn\n",
            "creating build/lib/opennre/module/pool\n",
            "copying opennre/module/pool/max_pool.py -> build/lib/opennre/module/pool\n",
            "copying opennre/module/pool/__init__.py -> build/lib/opennre/module/pool\n",
            "copying opennre/module/pool/avg_pool.py -> build/lib/opennre/module/pool\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/opennre\n",
            "copying build/lib/opennre/pretrain.py -> build/bdist.linux-x86_64/egg/opennre\n",
            "creating build/bdist.linux-x86_64/egg/opennre/tokenization\n",
            "copying build/lib/opennre/tokenization/bert_tokenizer.py -> build/bdist.linux-x86_64/egg/opennre/tokenization\n",
            "copying build/lib/opennre/tokenization/basic_tokenizer.py -> build/bdist.linux-x86_64/egg/opennre/tokenization\n",
            "copying build/lib/opennre/tokenization/__init__.py -> build/bdist.linux-x86_64/egg/opennre/tokenization\n",
            "copying build/lib/opennre/tokenization/word_piece_tokenizer.py -> build/bdist.linux-x86_64/egg/opennre/tokenization\n",
            "copying build/lib/opennre/tokenization/word_tokenizer.py -> build/bdist.linux-x86_64/egg/opennre/tokenization\n",
            "copying build/lib/opennre/tokenization/utils.py -> build/bdist.linux-x86_64/egg/opennre/tokenization\n",
            "creating build/bdist.linux-x86_64/egg/opennre/framework\n",
            "copying build/lib/opennre/framework/multi_label_sentence_re.py -> build/bdist.linux-x86_64/egg/opennre/framework\n",
            "copying build/lib/opennre/framework/data_loader.py -> build/bdist.linux-x86_64/egg/opennre/framework\n",
            "copying build/lib/opennre/framework/__init__.py -> build/bdist.linux-x86_64/egg/opennre/framework\n",
            "copying build/lib/opennre/framework/sentence_re.py -> build/bdist.linux-x86_64/egg/opennre/framework\n",
            "copying build/lib/opennre/framework/bag_re.py -> build/bdist.linux-x86_64/egg/opennre/framework\n",
            "copying build/lib/opennre/framework/utils.py -> build/bdist.linux-x86_64/egg/opennre/framework\n",
            "creating build/bdist.linux-x86_64/egg/opennre/module\n",
            "creating build/bdist.linux-x86_64/egg/opennre/module/nn\n",
            "copying build/lib/opennre/module/nn/rnn.py -> build/bdist.linux-x86_64/egg/opennre/module/nn\n",
            "copying build/lib/opennre/module/nn/cnn.py -> build/bdist.linux-x86_64/egg/opennre/module/nn\n",
            "copying build/lib/opennre/module/nn/__init__.py -> build/bdist.linux-x86_64/egg/opennre/module/nn\n",
            "copying build/lib/opennre/module/nn/lstm.py -> build/bdist.linux-x86_64/egg/opennre/module/nn\n",
            "copying build/lib/opennre/module/__init__.py -> build/bdist.linux-x86_64/egg/opennre/module\n",
            "creating build/bdist.linux-x86_64/egg/opennre/module/pool\n",
            "copying build/lib/opennre/module/pool/max_pool.py -> build/bdist.linux-x86_64/egg/opennre/module/pool\n",
            "copying build/lib/opennre/module/pool/__init__.py -> build/bdist.linux-x86_64/egg/opennre/module/pool\n",
            "copying build/lib/opennre/module/pool/avg_pool.py -> build/bdist.linux-x86_64/egg/opennre/module/pool\n",
            "copying build/lib/opennre/__init__.py -> build/bdist.linux-x86_64/egg/opennre\n",
            "creating build/bdist.linux-x86_64/egg/opennre/model\n",
            "copying build/lib/opennre/model/__init__.py -> build/bdist.linux-x86_64/egg/opennre/model\n",
            "copying build/lib/opennre/model/bag_attention.py -> build/bdist.linux-x86_64/egg/opennre/model\n",
            "copying build/lib/opennre/model/sigmoid_nn.py -> build/bdist.linux-x86_64/egg/opennre/model\n",
            "copying build/lib/opennre/model/bag_one.py -> build/bdist.linux-x86_64/egg/opennre/model\n",
            "copying build/lib/opennre/model/base_model.py -> build/bdist.linux-x86_64/egg/opennre/model\n",
            "copying build/lib/opennre/model/bag_average.py -> build/bdist.linux-x86_64/egg/opennre/model\n",
            "copying build/lib/opennre/model/softmax_nn.py -> build/bdist.linux-x86_64/egg/opennre/model\n",
            "creating build/bdist.linux-x86_64/egg/opennre/encoder\n",
            "copying build/lib/opennre/encoder/pcnn_encoder.py -> build/bdist.linux-x86_64/egg/opennre/encoder\n",
            "copying build/lib/opennre/encoder/cnn_encoder.py -> build/bdist.linux-x86_64/egg/opennre/encoder\n",
            "copying build/lib/opennre/encoder/bert_encoder.py -> build/bdist.linux-x86_64/egg/opennre/encoder\n",
            "copying build/lib/opennre/encoder/base_encoder.py -> build/bdist.linux-x86_64/egg/opennre/encoder\n",
            "copying build/lib/opennre/encoder/__init__.py -> build/bdist.linux-x86_64/egg/opennre/encoder\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/pretrain.py to pretrain.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/tokenization/bert_tokenizer.py to bert_tokenizer.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/tokenization/basic_tokenizer.py to basic_tokenizer.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/tokenization/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/tokenization/word_piece_tokenizer.py to word_piece_tokenizer.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/tokenization/word_tokenizer.py to word_tokenizer.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/tokenization/utils.py to utils.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/framework/multi_label_sentence_re.py to multi_label_sentence_re.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/framework/data_loader.py to data_loader.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/framework/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/framework/sentence_re.py to sentence_re.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/framework/bag_re.py to bag_re.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/framework/utils.py to utils.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/nn/rnn.py to rnn.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/nn/cnn.py to cnn.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/nn/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/nn/lstm.py to lstm.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/pool/max_pool.py to max_pool.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/pool/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/pool/avg_pool.py to avg_pool.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/model/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/model/bag_attention.py to bag_attention.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/model/sigmoid_nn.py to sigmoid_nn.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/model/bag_one.py to bag_one.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/model/base_model.py to base_model.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/model/bag_average.py to bag_average.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/model/softmax_nn.py to softmax_nn.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/encoder/pcnn_encoder.py to pcnn_encoder.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/encoder/cnn_encoder.py to cnn_encoder.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/encoder/bert_encoder.py to bert_encoder.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/encoder/base_encoder.py to base_encoder.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/encoder/__init__.py to __init__.cpython-37.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying opennre.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying opennre.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying opennre.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying opennre.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/opennre-0.1-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing opennre-0.1-py3.7.egg\n",
            "Copying opennre-0.1-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n",
            "Adding opennre 0.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/opennre-0.1-py3.7.egg\n",
            "Processing dependencies for opennre==0.1\n",
            "Finished processing dependencies for opennre==0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTzkmfalfSXp"
      },
      "source": [
        "### Get relation extraction model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgHACmSceV1d",
        "outputId": "0d417ea8-877d-45f8-85ee-6153710239bf"
      },
      "source": [
        "import opennre\n",
        "model = opennre.get_model('wiki80_bert_softmax')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-12-01 02:37:48,651 - root - INFO - Loading BERT pre-trained checkpoint.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhqgKf66X39M"
      },
      "source": [
        "## Extract Triplets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAZACmjOwqi-",
        "outputId": "e205c606-5d9a-4da0-c187-baa1b6d25c8f"
      },
      "source": [
        "nlp.entity.labels"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('NORP',\n",
              " 'EVENT',\n",
              " 'WORK_OF_ART',\n",
              " 'CARDINAL',\n",
              " 'TIME',\n",
              " 'ORDINAL',\n",
              " 'GPE',\n",
              " 'LANGUAGE',\n",
              " 'ORG',\n",
              " 'PERCENT',\n",
              " 'DATE',\n",
              " 'LAW',\n",
              " 'PRODUCT',\n",
              " 'QUANTITY',\n",
              " 'LOC',\n",
              " 'PERSON',\n",
              " 'MONEY',\n",
              " 'FAC')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXMlzb4zEmw7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f09a75-4dd4-4358-ab34-8d1fafcb6339"
      },
      "source": [
        "sentences = [sent.text for sent in nlp(coref_resolved_spr_wiki_text).sents]\n",
        "print(\"The number of sentences to extract triplets:\", len(sentences))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of sentences to extract triplets: 271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS07l66XDYQG"
      },
      "source": [
        "def extract_triplets(doc):\n",
        "  # update tokenizer\n",
        "  doc = update_tokenizer(doc)\n",
        "\n",
        "  # apply rules to extract triplets\n",
        "  rule_1_triples = rule_1(doc)\n",
        "  rule_2_triples = rule_2(doc)\n",
        "  rule_3_triples = rule_3(doc)\n",
        "  rule_4_triples = rule_4(doc)\n",
        "\n",
        "  return set([*rule_1_triples, *rule_2_triples, *rule_3_triples, *rule_4_triples])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvCyHepEfAhm"
      },
      "source": [
        "from itertools import combinations\n",
        "def extract_triples_opennre(model, doc, threshold):\n",
        "  triples = []\n",
        "\n",
        "  # update tokenizer\n",
        "  doc = update_tokenizer(doc)\n",
        "\n",
        "  entities = get_named_entities(doc)\n",
        "  if len(entities) < 2:\n",
        "    return triples\n",
        "  combs = combinations(entities, 2)\n",
        "  for comb in combs:\n",
        "    entity_1, entity_2 = comb\n",
        "    relation = model.infer({'text': doc.text, 'h': {'pos': entity_1[2]}, 't': {'pos': entity_2[2]}})\n",
        "    # print(entity_1[0], relation, entity_2[0])\n",
        "    if relation[1] >= threshold:\n",
        "      triples.append((entity_1[0], relation[0], entity_2[0]))\n",
        "  return triples"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtRFprGkMV-W",
        "outputId": "c7c544df-811b-4a52-8afb-fe541fb6c668"
      },
      "source": [
        "%cd /content"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6cv-yYFLJs8",
        "outputId": "fc573209-ee0e-4895-cd5d-2f7fcb9e5eff"
      },
      "source": [
        "with open('triplets.txt', 'w') as fw:\n",
        "  for sentence in sentences:\n",
        "    doc = nlp(sentence)\n",
        "    triplets = extract_triplets(doc)\n",
        "    opennre = extract_triples_opennre(model, doc, 0.8)\n",
        "    for triplet in triplets:\n",
        "      fw.write(str(triplet) + '\\n')\n",
        "\n",
        "    for triplet in opennre:\n",
        "      fw.write(str(triplet) + '\\n')\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Swanton Pacific Ranch ('has part', 0.9012349843978882) Santa Cruz County\n",
            "Swanton Pacific Ranch ('said to be the same as', 0.5751903057098389) California\n",
            "Swanton Pacific Ranch ('subsidiary', 0.44678452610969543) Davenport\n",
            "Santa Cruz County ('has part', 0.5785011649131775) California\n",
            "Santa Cruz County ('has part', 0.48109742999076843) Davenport\n",
            "California ('subsidiary', 0.5553775429725647) Davenport\n",
            "Swanton Pacific Ranch ('operator', 0.590891420841217) California Polytechnic State University\n",
            "Swanton Pacific Ranch ('has part', 0.7554218769073486) the College of Agriculture, Food and Environmental Sciences\n",
            "Swanton Pacific Ranch ('followed by', 0.4934234321117401) Swanton\n",
            "the College of Agriculture, Food and Environmental Sciences ('followed by', 0.4124290347099304) Swanton\n",
            "Waddell Creek ('has part', 0.8398447632789612) the mid 19th century\n",
            "November 1843 ('followed by', 0.9910421371459961) Ramon Rodriguez\n",
            "November 1843 ('location', 0.17697328329086304) Francisco Alviso\n",
            "November 1843 ('follows', 0.3016408383846283) California\n",
            "November 1843 ('follows', 0.6126365065574646) Manuel Micheltorena\n",
            "Ramon Rodriguez ('follows', 0.6164525747299194) Francisco Alviso\n",
            "Ramon Rodriguez ('follows', 0.4981878101825714) California\n",
            "Ramon Rodriguez ('followed by', 0.7460224628448486) Manuel Micheltorena\n",
            "Francisco Alviso ('follows', 0.9342966079711914) California\n",
            "Francisco Alviso ('follows', 0.7637141942977905) Manuel Micheltorena\n",
            "California ('follows', 0.6362086534500122) Manuel Micheltorena\n",
            "Rancho Agua Puerca ('has part', 0.5993216633796692) Hog Water and the Bars\n",
            "Rancho Agua Puerca ('has part', 0.2858757972717285) March 1, 1867\n",
            "Hog Water and the Bars ('said to be the same as', 0.9963447451591492) March 1, 1867\n",
            "The same year The same year ('said to be the same as', 0.9317495226860046) James Archibald\n",
            "James Archibald ('said to be the same as', 0.612386167049408) Ambrogio Gianone\n",
            "James Archibald ('sibling', 0.5031239986419678) Swiss\n",
            "James Archibald ('residence', 0.4329274892807007) 1867\n",
            "James Archibald ('residence', 0.4743311405181885) 1880\n",
            "Ambrogio Gianone ('subsidiary', 0.8519849181175232) Swiss\n",
            "Ambrogio Gianone ('said to be the same as', 0.6519820094108582) 1867\n",
            "Ambrogio Gianone ('instance of', 0.47858259081840515) 1880\n",
            "Swiss ('said to be the same as', 0.1850374937057495) 1867\n",
            "Swiss ('instance of', 0.6795067191123962) 1880\n",
            "1867 ('located in the administrative territorial entity', 0.9885977506637573) 1880\n",
            "Gianone ('member of', 0.6924561858177185) a third\n",
            "Gianone ('owned by', 0.5602255463600159) Gianone Hill\n",
            "a third ('owned by', 0.8786158561706543) Gianone Hill\n",
            "James Archibald ('spouse', 0.9675605893135071) 1875\n",
            "James Archibald ('spouse', 0.9975391626358032) James Archibald wife\n",
            "James Archibald ('spouse', 0.998513400554657) Joseph Bloom\n",
            "1875 ('spouse', 0.9954996705055237) James Archibald wife\n",
            "1875 ('spouse', 0.9954351782798767) Joseph Bloom\n",
            "James Archibald wife ('spouse', 0.9878653883934021) Joseph Bloom\n",
            "Santa Cruz ('member of political party', 0.6800437569618225) Fred Swanton\n",
            "the last decade of the 1800s ('followed by', 0.33039072155952454) two dams\n",
            "the last decade of the 1800s ('participant', 0.38531213998794556) Big Creek\n",
            "the last decade of the 1800s ('sibling', 0.9349623918533325) Mill Creek\n",
            "the last decade of the 1800s ('followed by', 0.8322376012802124) the Central Coast Counties Gas and Electric\n",
            "the last decade of the 1800s ('followed by', 0.38468119502067566) PG&E.\n",
            "two dams ('follows', 0.770858645439148) Big Creek\n",
            "two dams ('has part', 0.4057929217815399) Mill Creek\n",
            "two dams ('follows', 0.6026659607887268) the Central Coast Counties Gas and Electric\n",
            "two dams ('follows', 0.8756341338157654) PG&E.\n",
            "Big Creek ('followed by', 0.3659980297088623) Mill Creek\n",
            "Big Creek ('follows', 0.8283793926239014) the Central Coast Counties Gas and Electric\n",
            "Big Creek ('follows', 0.8095223307609558) PG&E.\n",
            "Mill Creek ('has part', 0.6830607056617737) the Central Coast Counties Gas and Electric\n",
            "Mill Creek ('has part', 0.7122975587844849) PG&E.\n",
            "the Central Coast Counties Gas and Electric ('followed by', 0.6356208324432373) PG&E.\n",
            "the Ocean Shore Railroad ('followed by', 0.5454407930374146) San Francisco\n",
            "the Ocean Shore Railroad ('followed by', 0.68044114112854) Santa Cruz\n",
            "San Francisco ('followed by', 0.964372992515564) Santa Cruz\n",
            "the Ocean Shore Railroad ('has part', 0.3281128704547882) Swanton\n",
            "the Ocean Shore Railroad ('said to be the same as', 0.7716768980026245) the San Vincente Lumber Company sawmill\n",
            "the Ocean Shore Railroad ('owned by', 0.8440157175064087) Santa Cruz\n",
            "Swanton ('followed by', 0.680878221988678) the San Vincente Lumber Company sawmill\n",
            "Swanton ('operator', 0.4296194314956665) Santa Cruz\n",
            "the San Vincente Lumber Company sawmill ('followed by', 0.5785577893257141) Santa Cruz\n",
            "1922 ('followed by', 0.9474048614501953) the Ocean Shore Railroad\n",
            "1922 ('followed by', 0.9221044182777405) the Ocean Shore Railroad employees\n",
            "the Ocean Shore Railroad ('follows', 0.5588420629501343) the Ocean Shore Railroad employees\n",
            "the 1920s ('has part', 0.6343758702278137) Brussels\n",
            "Poletti ('sibling', 0.5821183919906616) Morelli\n",
            "Poletti ('sibling', 0.9344213604927063) 1938\n",
            "Morelli ('manufacturer', 0.7715868949890137) 1938\n",
            "Swanton Pacific Ranch ('manufacturer', 0.31525006890296936) three phases\n",
            "John ('military rank', 0.4199220836162567) Bob Musitelli\n",
            "Muistelli ('owned by', 0.4326536953449249) John\n",
            "Muistelli ('owned by', 0.9801114201545715) Bob Musitelli\n",
            "John ('owned by', 0.13283975422382355) Bob Musitelli\n",
            "1978 ('followed by', 0.6022566556930542) Albert B. Smith\n",
            "1978 ('follows', 0.2856614291667938) Orchard Supply Hardware\n",
            "1978 ('successful candidate', 0.5349492430686951) Swanton Pacific Ranch\n",
            "Albert B. Smith ('publisher', 0.4286762475967407) Orchard Supply Hardware\n",
            "Albert B. Smith ('successful candidate', 0.6560503840446472) Swanton Pacific Ranch\n",
            "Orchard Supply Hardware ('owned by', 0.48156100511550903) Swanton Pacific Ranch\n",
            "Smith ('said to be the same as', 0.4990762770175934) first\n",
            "Boy Scout camp ('member of', 0.6737499237060547) Smith\n",
            "Boy Scout camp ('has part', 0.6371047496795654) Swanton Pacific Ranch\n",
            "Smith ('has part', 0.43387165665626526) Swanton Pacific Ranch\n",
            "1993 ('followed by', 0.3191838264465332) Smith\n",
            "1993 ('followed by', 0.15867356956005096) Swanton Pacific Ranch\n",
            "1993 ('owned by', 0.6102514863014221) Swanton\n",
            "Smith ('follows', 0.8494741916656494) Swanton Pacific Ranch\n",
            "Smith ('owned by', 0.7977738380432129) Swanton\n",
            "Swanton Pacific Ranch ('owned by', 0.5072348713874817) Swanton\n",
            "Swanton Pacific Ranch ('tributary', 0.37859323620796204) Santa Cruz\n",
            "Scott Creek ('tributary', 0.975850522518158) Swanton Pacific Ranch\n",
            "Coho salmon ('tributary', 0.8043457865715027) Steelhead trout\n",
            "Swanton Pacific's ('taxon rank', 0.8706150650978088) Northern California Forest\n",
            "Swanton Pacific ('subsidiary', 0.5222089290618896) Cal Poly's\n",
            "Swanton Pacific ('subsidiary', 0.9954912066459656) the 4,600 acres\n",
            "Cal Poly's ('subsidiary', 0.9845290780067444) the 4,600 acres\n",
            "Swanton Pacific Ranch ('has part', 0.44190922379493713) the Pacific Ocean\n",
            "Swanton Pacific Ranch ('licensed to broadcast to', 0.2832525968551636) fifteen miles\n",
            "Swanton Pacific Ranch ('tributary', 0.35731419920921326) Santa Cruz\n",
            "fifteen miles ('followed by', 0.6068270802497864) Santa Cruz\n",
            "around 1,200 feet ('has part', 0.9411715269088745) Scott Creek\n",
            "Scotts Creek ('tributary', 0.847214937210083) Mill Creek\n",
            "Scotts Creek ('tributary', 0.9599919319152832) Little Creek\n",
            "Scotts Creek ('has part', 0.8952497839927673) Archibald Creek\n",
            "Scotts Creek ('has part', 0.8708853721618652) Queseria Creek\n",
            "Mill Creek ('taxon rank', 0.7340772151947021) Little Creek\n",
            "Mill Creek ('follows', 0.5381959676742554) Archibald Creek\n",
            "Mill Creek ('has part', 0.6572324633598328) Queseria Creek\n",
            "Little Creek ('tributary', 0.7616361379623413) Archibald Creek\n",
            "Little Creek ('has part', 0.5659304857254028) Queseria Creek\n",
            "Archibald Creek ('has part', 0.4212243854999542) Queseria Creek\n",
            "Scotts Creek ('has part', 0.6898233890533447) Mill Creek\n",
            "Scotts Creek ('part of', 0.39633801579475403) Little Creek\n",
            "Scotts Creek ('taxon rank', 0.8646461963653564) Archibald Creek\n",
            "Scotts Creek ('has part', 0.37783515453338623) Queseria Creek\n",
            "Scotts Creek ('tributary', 0.4210021197795868) Central California Coast\n",
            "Mill Creek ('has part', 0.4549591541290283) Little Creek\n",
            "Mill Creek ('taxon rank', 0.39383721351623535) Archibald Creek\n",
            "Mill Creek ('followed by', 0.7092111110687256) Queseria Creek\n",
            "Mill Creek ('has part', 0.6685106754302979) Central California Coast\n",
            "Little Creek ('taxon rank', 0.9511648416519165) Archibald Creek\n",
            "Little Creek ('taxon rank', 0.9575366377830505) Queseria Creek\n",
            "Little Creek ('has part', 0.42294302582740784) Central California Coast\n",
            "Archibald Creek ('taxon rank', 0.9702438712120056) Queseria Creek\n",
            "Archibald Creek ('tributary', 0.7688463926315308) Central California Coast\n",
            "Queseria Creek ('tributary', 0.8504791855812073) Central California Coast\n",
            "Almost half ('said to be the same as', 0.5694332122802734) Swanton Pacific Ranch\n",
            "Almost half ('owned by', 0.4842132329940796) 1,355 acres\n",
            "Swanton Pacific Ranch ('has part', 0.495385080575943) 1,355 acres\n",
            "Scotts Creek ('has part', 0.7452008128166199) Mill Creek\n",
            "Scotts Creek ('tributary', 0.5807609558105469) Little Creek\n",
            "Scotts Creek ('has part', 0.71999192237854) Archibald Creek\n",
            "Scotts Creek ('has part', 0.9624807834625244) Queseria Creek\n",
            "Scotts Creek ('has part', 0.8398228287696838) only a few feet\n",
            "Mill Creek ('tributary', 0.9279348254203796) Little Creek\n",
            "Mill Creek ('part of', 0.8922439217567444) Archibald Creek\n",
            "Mill Creek ('has part', 0.9263707995414734) Queseria Creek\n",
            "Mill Creek ('has part', 0.8062393665313721) only a few feet\n",
            "Little Creek ('has part', 0.4104498326778412) Archibald Creek\n",
            "Little Creek ('followed by', 0.6818408966064453) Queseria Creek\n",
            "Little Creek ('has part', 0.4221823513507843) only a few feet\n",
            "Archibald Creek ('has part', 0.9390195608139038) Queseria Creek\n",
            "Archibald Creek ('instance of', 0.583464503288269) only a few feet\n",
            "Queseria Creek ('has part', 0.9377956390380859) only a few feet\n",
            "Ranch ('followed by', 0.7381386160850525) The Red House\n",
            "The Red House ('has part', 0.17197254300117493) two meeting rooms\n",
            "The Red House ('residence', 0.7105036377906799) one large room\n",
            "two meeting rooms ('has part', 0.14644768834114075) one large room\n",
            "The Red House ('has part', 0.44753167033195496) five bedrooms\n",
            "The Red House ('has part', 0.7498269081115723) two\n",
            "five bedrooms ('manufacturer', 0.6187039017677307) two\n",
            "The Red House ('subsidiary', 0.3171679675579071) Swanton Road\n",
            "The Red House ('has part', 0.8207718133926392) Cal Poly SLO\n",
            "Swanton Road ('headquarters location', 0.37168923020362854) Cal Poly SLO\n",
            "The Red House ('said to be the same as', 0.6636834144592285) The Red House\n",
            "The Red House ('architect', 0.3011265993118286) The Red House frequent visits\n",
            "The Red House ('said to be the same as', 0.3004467785358429) The Red House frequent visits\n",
            "The Red House ('said to be the same as', 0.24871954321861267) two bedrooms\n",
            "The Red House ('sibling', 0.667596697807312) two\n",
            "two bedrooms ('residence', 0.7194016575813293) two\n",
            "Swanton Pacific Ranch ('said to be the same as', 0.5095037221908569) 1874\n",
            "Swanton Ranch ('part of', 0.7098599076271057) Swanton Pacific Ranch's\n",
            "Cal Poly SLO ('manufacturer', 0.9411778450012207) Swanton Ranch\n",
            "Cal Poly SLO ('manufacturer', 0.9609718918800354) Cal Poly SLO\n",
            "Swanton Ranch ('manufacturer', 0.9620419144630432) Cal Poly SLO\n",
            "The Red House ('has part', 0.45798343420028687) Swanton Pacific Ranch\n",
            "The Red House ('has part', 0.4017341434955597) ten students\n",
            "The Red House ('has part', 0.44753167033195496) five bedrooms\n",
            "The Red House ('has part', 0.7498269081115723) two\n",
            "five bedrooms ('manufacturer', 0.6187039017677307) two\n",
            "The Red House ('said to be the same as', 0.7371248602867126) one\n",
            "The Red House ('said to be the same as', 0.9808233976364136) Swanton Pacific Ranch\n",
            "one ('said to be the same as', 0.6422320604324341) Swanton Pacific Ranch\n",
            "Yurts ('said to be the same as', 0.6589071750640869) The Red House\n",
            "Red House ('sibling', 0.29957714676856995) forty people\n",
            "Red House ('has part', 0.23275519907474518) Swanton Pacific Ranch\n",
            "forty people ('has part', 0.3412095308303833) Swanton Pacific Ranch\n",
            "Yurts ('followed by', 0.3343774378299713) the Staub House\n",
            "Yurts ('sibling', 0.6789941787719727) 1999\n",
            "the Staub House ('followed by', 0.9929919242858887) 1999\n",
            "the Cheese House ('has part', 0.9512014985084534) Swanton Pacific Ranch\n",
            "19 ('follows', 0.6741812229156494) the 1980s\n",
            "19 ('manufacturer', 0.9932621121406555) Albert Smith\n",
            "the 1980s ('manufacturer', 0.9886497259140015) Albert Smith\n",
            "The Red House ('sibling', 0.7419174313545227) 1/3 standard size\n",
            "The Red House ('said to be the same as', 0.23305316269397736) four miniature steam locomotives\n",
            "The Red House ('followed by', 0.5360135436058044) Louis MacDermot\n",
            "The Red House ('followed by', 0.8258111476898193) the Overfair Railway\n",
            "1/3 standard size ('follows', 0.5109701156616211) four miniature steam locomotives\n",
            "1/3 standard size ('follows', 0.7577117681503296) Louis MacDermot\n",
            "1/3 standard size ('followed by', 0.628649115562439) the Overfair Railway\n",
            "four miniature steam locomotives ('follows', 0.6715656518936157) Louis MacDermot\n",
            "four miniature steam locomotives ('has part', 0.5959413647651672) the Overfair Railway\n",
            "Louis MacDermot ('follows', 0.8982172608375549) the Overfair Railway\n",
            "the Overfair Railway ('has part', 0.7061125636100769) the Panama-Pacific International Exposition of\n",
            "the Overfair Railway ('followed by', 0.8795918226242065) 1915\n",
            "the Overfair Railway ('has part', 0.8560027480125427) the Panama Canal\n",
            "the Panama-Pacific International Exposition of ('has part', 0.8211528062820435) 1915\n",
            "the Panama-Pacific International Exposition of ('has part', 0.9643594622612) the Panama Canal\n",
            "1915 ('has part', 0.5174623131752014) the Panama Canal\n",
            "the Panama-Pacific International Exposition of ('has part', 0.5878378748893738) the Panama Canal\n",
            "the Panama-Pacific International Exposition of ('followed by', 0.41572144627571106) the Overfair Railway\n",
            "the Panama Canal ('sibling', 0.4555617868900299) the Overfair Railway\n",
            "four ('followed by', 0.736574649810791) Louis MacDermot\n",
            "four ('followed by', 0.9462177157402039) the Overfair Railway\n",
            "four ('followed by', 0.6988103985786438) MacDermot\n",
            "four ('follows', 0.6842530369758606) the 1980s\n",
            "Louis MacDermot ('followed by', 0.9773131608963013) the Overfair Railway\n",
            "Louis MacDermot ('follows', 0.5891172885894775) MacDermot\n",
            "Louis MacDermot ('follows', 0.6051734089851379) the 1980s\n",
            "the Overfair Railway ('follows', 0.7397462129592896) MacDermot\n",
            "the Overfair Railway ('follows', 0.7119453549385071) the 1980s\n",
            "MacDermot ('follows', 0.8525896072387695) the 1980s\n",
            "Albert Smith ('manufacturer', 0.5477021932601929) four miniature steam locomotives\n",
            "Albert Smith ('owned by', 0.5347880721092224) Louis MacDermot\n",
            "Albert Smith ('has part', 0.1962975710630417) the Overfair Railway\n",
            "Albert Smith ('manufacturer', 0.8792304992675781) Albert Smith love\n",
            "four miniature steam locomotives ('followed by', 0.30332162976264954) Louis MacDermot\n",
            "four miniature steam locomotives ('has part', 0.8098269701004028) the Overfair Railway\n",
            "four miniature steam locomotives ('follows', 0.6197908520698547) Albert Smith love\n",
            "Louis MacDermot ('has part', 0.8235311508178711) the Overfair Railway\n",
            "Louis MacDermot ('has part', 0.3172270655632019) Albert Smith love\n",
            "the Overfair Railway ('follows', 0.7422633767127991) Albert Smith love\n",
            "1979 ('followed by', 0.8830666542053223) Albert Smith\n",
            "1979 ('followed by', 0.5284698605537415) Scotts Creek\n",
            "Albert Smith ('successful candidate', 0.8038234114646912) Scotts Creek\n",
            "Today ('operator', 0.9263189435005188) the Swanton Pacific Railroad Society\n",
            "Today ('followed by', 0.30679333209991455) Cal Poly\n",
            "the Swanton Pacific Railroad Society ('owned by', 0.4945761561393738) Cal Poly\n",
            "Swanton Pacific Ranch Swanton Pacific Ranch ('subsidiary', 0.894227147102356) 80%\n",
            "The remaining 20% ('has part', 0.7695398330688477) Albert Smith\n",
            "The remaining 20% ('has part', 0.8356456756591797) Swanton Pacific Ranch\n",
            "The remaining 20% ('part of', 0.34197676181793213) Cal Poly SLO\n",
            "Albert Smith ('developer', 0.40211594104766846) Swanton Pacific Ranch\n",
            "Albert Smith ('subsidiary', 0.7147300243377686) Cal Poly SLO\n",
            "Swanton Pacific Ranch ('follows', 0.24669113755226135) Cal Poly SLO\n",
            "Swanton Pacific Ranch ('operator', 0.970156192779541) California\n",
            "Swanton Pacific Ranch ('manufacturer', 0.242728590965271) the California Polytechnic State University\n",
            "California ('part of', 0.5607346892356873) the California Polytechnic State University\n",
            "Swanton Pacific Ranch Swanton Pacific Ranch ('has part', 0.6143910884857178) K-12th grade\n",
            "Cal Poly SLO ('subsidiary', 0.7372883558273315) Forest Management\n",
            "Scott Creek ('has part', 0.7147039771080017) Swanton Pacific\n",
            "Valencia Creek ('followed by', 0.6287633776664734) Swanton Pacific\n",
            "Redwood harvest ('said to be the same as', 0.6180593967437744) the early 1900s\n",
            "first ('followed by', 0.5850253701210022) between 1907 and 1911\n",
            "Forty acres ('has part', 0.8461205959320068) Redwood Redwood\n",
            "Forty acres ('manufacturer', 0.7825945019721985) the 1960s\n",
            "Redwood Redwood ('has part', 0.31823211908340454) the 1960s\n",
            "the 40 acres ('said to be the same as', 0.7889395952224731) second growth Redwood\n",
            "the 40 acres ('has part', 0.6344592571258545) approximately 80 years old\n",
            "the 40 acres ('has part', 0.9801509380340576) Douglas fir\n",
            "the 40 acres ('has part', 0.8094808459281921) Redwood trees\n",
            "second growth Redwood ('has part', 0.8660599589347839) approximately 80 years old\n",
            "second growth Redwood ('has part', 0.9659850001335144) Douglas fir\n",
            "second growth Redwood ('has part', 0.8127068281173706) Redwood trees\n",
            "approximately 80 years old ('has part', 0.9544214010238647) Douglas fir\n",
            "approximately 80 years old ('follows', 0.546303391456604) Redwood trees\n",
            "Douglas fir ('has part', 0.6956226825714111) Redwood trees\n",
            "Kelly ('developer', 0.35001489520072937) Anderson\n",
            "Kelly ('followed by', 0.2869113087654114) 1989\n",
            "Anderson ('followed by', 0.9562191963195801) 1989\n",
            "Kelly ('has part', 0.27533817291259766) Anderson\n",
            "Kelly ('has part', 0.36273911595344543) 1989\n",
            "Kelly ('has part', 0.9106407761573792) Redwood and 17,318bdft/acre\n",
            "Kelly ('has part', 0.8824620246887207) Douglas fir\n",
            "Anderson ('followed by', 0.43059876561164856) 1989\n",
            "Anderson ('has part', 0.4516713321208954) Redwood and 17,318bdft/acre\n",
            "Anderson ('has part', 0.4235621988773346) Douglas fir\n",
            "1989 ('has part', 0.924026370048523) Redwood and 17,318bdft/acre\n",
            "1989 ('has part', 0.427449494600296) Douglas fir\n",
            "Redwood and 17,318bdft/acre ('manufacturer', 0.2027425616979599) Douglas fir\n",
            "Kelly ('subsidiary', 0.9159775972366333) Anderson\n",
            "Kelly ('subsidiary', 0.9701552987098694) 1989\n",
            "Kelly ('member of', 0.3681350648403168) 15 acre CFI plots\n",
            "Anderson ('followed by', 0.7819970846176147) 1989\n",
            "Anderson ('followed by', 0.6239227056503296) 15 acre CFI plots\n",
            "1989 ('manufacturer', 0.5977042317390442) 15 acre CFI plots\n",
            "15 ('has part', 0.4948948919773102) 1997\n",
            "15 ('part of', 0.6634483933448792) Larry Bonner\n",
            "1997 ('follows', 0.5562984943389893) Larry Bonner\n",
            "26,163bdft/acre for Redwood and 17,318bdft/acre ('follows', 0.8614818453788757) Douglas fir\n",
            "26,163bdft/acre for Redwood and 17,318bdft/acre ('follows', 0.7241557240486145) Redwood\n",
            "26,163bdft/acre for Redwood and 17,318bdft/acre ('follows', 0.877659797668457) 8,923bdft\n",
            "26,163bdft/acre for Redwood and 17,318bdft/acre ('follows', 0.5618723630905151) Douglas fir\n",
            "Douglas fir ('main subject', 0.39440688490867615) Redwood\n",
            "Douglas fir ('main subject', 0.9228029251098633) 8,923bdft\n",
            "Douglas fir ('main subject', 0.3686037063598633) Douglas fir\n",
            "Redwood ('developer', 0.40139299631118774) 8,923bdft\n",
            "Redwood ('subsidiary', 0.8496744632720947) Douglas fir\n",
            "8,923bdft ('follows', 0.3770322799682617) Douglas fir\n",
            "approximately 76% Redwood ('follows', 0.5690047144889832) 24%\n",
            "approximately 76% Redwood ('operator', 0.4659273326396942) Douglas fir\n",
            "approximately 76% Redwood ('follows', 0.4130937457084656) 200,000\n",
            "24% ('follows', 0.8422374725341797) Douglas fir\n",
            "24% ('follows', 0.8568404912948608) 200,000\n",
            "Douglas fir ('follows', 0.7837356328964233) 200,000\n",
            "second ('main subject', 0.37309056520462036) 1993-95\n",
            "second ('followed by', 0.8252385854721069) 542,803\n",
            "1993-95 ('main subject', 0.37649214267730713) 542,803\n",
            "200,000 ('follows', 0.6664429903030396) the years\n",
            "259 ('owned by', 0.5441340804100037) the last fifteen years\n",
            "One ('has part', 0.4787243902683258) the next two years\n",
            "ten to fifteen year periods ('subsidiary', 0.8516187071800232) five years\n",
            "Monterey pines ('has part', 0.8000709414482117) The Scott's Creek Forest unit\n",
            "Cal Poly ('architect', 0.35761478543281555) Scott Creek\n",
            "Santa Cruz County ('contains administrative territorial entity', 0.6169266104698181) Santa Cruz County zoning regulations\n",
            "Santa Cruz County ('headquarters location', 0.28397393226623535) The Scott's Creek Forest unit\n",
            "Santa Cruz County zoning regulations ('contains administrative territorial entity', 0.6203953623771667) The Scott's Creek Forest unit\n",
            "Santa Cruz County ('main subject', 0.3422259986400604) 2001\n",
            "November 1999 ('follows', 0.824653685092926) a Continuous Forestry Inventory\n",
            "November 1999 ('manufacturer', 0.9211885333061218) Steve Auten\n",
            "November 1999 ('developer', 0.33665579557418823) January 2000\n",
            "a Continuous Forestry Inventory ('follows', 0.5211279988288879) Steve Auten\n",
            "a Continuous Forestry Inventory ('developer', 0.44584643840789795) January 2000\n",
            "Steve Auten ('developer', 0.4874171018600464) January 2000\n",
            "54.9 acres ('said to be the same as', 0.8325760364532471) Monterey pine\n",
            "54.9 acres ('operator', 0.7116137742996216) 207.1 acres\n",
            "54.9 acres ('follows', 0.3498416841030121) RedwoodDouglas fir mix\n",
            "Monterey pine ('manufacturer', 0.4592841863632202) 207.1 acres\n",
            "Monterey pine ('manufacturer', 0.8125563859939575) RedwoodDouglas fir mix\n",
            "207.1 acres ('has part', 0.30477872490882874) RedwoodDouglas fir mix\n",
            "1955 ('operator', 0.26879385113716125) The Scott's Creek Forest unit\n",
            "Albert Smith ('has part', 0.6477702260017395) 1993\n",
            "Albert Smith ('has part', 0.54057377576828) Albert Smith\n",
            "Albert Smith ('manufacturer', 0.8916658759117126) Swanton Pacific Ranch\n",
            "1993 ('has part', 0.9116411209106445) Albert Smith\n",
            "1993 ('has part', 0.5158608555793762) Swanton Pacific Ranch\n",
            "Albert Smith ('has part', 0.9746413826942444) Swanton Pacific Ranch\n",
            "Tan oak ('followed by', 0.6434994339942932) Madrone\n",
            "Tan oak ('said to be the same as', 0.8575171232223511) Shreve oak\n",
            "Madrone ('said to be the same as', 0.5696795582771301) Shreve oak\n",
            "two subsequent harvests ('followed by', 0.7289267778396606) the 1960s and 1970s\n",
            "two ('followed by', 0.7111020684242249) 1970s\n",
            "Valencia stand ('said to be the same as', 0.3709087371826172) approximately one hundred years old\n",
            "seven smaller and less marketable stands ('manufacturer', 0.4631357789039612) Douglas fir\n",
            "An estimated 15,340,000 board feet ('has part', 0.39371854066848755) Valencia Creek\n",
            "An estimated 15,340,000 board feet ('followed by', 0.23725567758083344) Swanton Pacific\n",
            "Valencia Creek ('operator', 0.5093587040901184) Swanton Pacific\n",
            "Only approximately 3,000,000 ('followed by', 0.7622848749160767) An estimated 15,340,000 board feet\n",
            "Only approximately 3,000,000 ('has part', 0.9258399605751038) the next twenty years\n",
            "An estimated 15,340,000 board feet ('sibling', 0.46202635765075684) the next twenty years\n",
            "Swanton Pacific Ranch ('manufacturer', 0.5330421328544617) GIS technology\n",
            "Swanton Pacific Ranch ('has part', 0.43545427918434143) Coho Salmon\n",
            "Swanton Pacific Ranch ('part of', 0.2627130150794983) 75-to-200-foot buffer\n",
            "Coho Salmon ('has part', 0.5440333485603333) 75-to-200-foot buffer\n",
            "Douglas fir ('taxon rank', 0.6062416434288025) Douglas-fir tussock moth\n",
            "Monterey Pine ('taxon rank', 0.8298458456993103) two endemic pests\n",
            "One ('has part', 0.4419400095939636) Swanton\n",
            "One ('has part', 0.9059227108955383) the Animal Science Department\n",
            "One ('has part', 0.9342638254165649) Cal Poly SLO\n",
            "Swanton ('part of', 0.7207230925559998) the Animal Science Department\n",
            "Swanton ('occupant', 0.39178988337516785) Cal Poly SLO\n",
            "the Animal Science Department ('subsidiary', 0.9318943023681641) Cal Poly SLO\n",
            "the Animal Science Department ('subsidiary', 0.9951162338256836) Swanton Pacific Ranch\n",
            "the Animal Science Department ('subsidiary', 0.9971709847450256) the Animal Science Department\n",
            "the Animal Science Department ('subsidiary', 0.9974684715270996) twice a year\n",
            "Swanton Pacific Ranch ('subsidiary', 0.8796346187591553) the Animal Science Department\n",
            "Swanton Pacific Ranch ('subsidiary', 0.9958904981613159) twice a year\n",
            "the Animal Science Department ('subsidiary', 0.9959843158721924) twice a year\n",
            "approximately 115 acres orchard ('has part', 0.8810674548149109) five short tons\n",
            "Santa Cruz County ('has part', 0.472208172082901) State\n",
            "Santa Cruz County ('followed by', 0.3941197991371155) California\n",
            "Santa Cruz County ('contains administrative territorial entity', 0.8900726437568665) State\n",
            "Santa Cruz County ('has part', 0.5379067659378052) California\n",
            "State ('follows', 0.19181343913078308) California\n",
            "State ('contains administrative territorial entity', 0.9829913377761841) State\n",
            "State ('subsidiary', 0.4574274718761444) California\n",
            "California ('contains administrative territorial entity', 0.9903357028961182) State\n",
            "California ('has part', 0.3492872416973114) California\n",
            "State ('has part', 0.4070826768875122) California\n",
            "Five years ('has part', 0.6103126406669617) 2004\n",
            "Geomorphic ('followed by', 0.8912556767463684) Habitat Evaluation\n",
            "LIDAR Geomorphic Evaluation of Watershed and Channel Characteristics ('follows', 0.7969173789024353) the Little Creek Watershed\n",
            "LIDAR Geomorphic Evaluation of Watershed and Channel Characteristics ('follows', 0.7806832790374756) Swanton Pacific Ranch  \n",
            "the Little Creek Watershed ('follows', 0.6716389656066895) Swanton Pacific Ranch  \n",
            "LIDAR technology ('followed by', 0.9196444749832153) three\n",
            "LIDAR technology ('main subject', 0.3765299320220947) California Forest Practice Rules\n",
            "Monterey Pine  pitch canker ('member of', 0.6224453449249268) California Monterey pine forests\n",
            "California Polytechnic State University's ('subsidiary', 0.9903470873832703) Santa Cruz County\n",
            "the Animal Science Department ('subsidiary', 0.8186701536178589) 1\n",
            "the Animal Science Department ('subsidiary', 0.836531400680542) Swanton Pacific\n",
            "the Animal Science Department ('developer', 0.14394430816173553) 2\n",
            "the Animal Science Department ('subsidiary', 0.9831891059875488) Monterey pine forests\n",
            "the Animal Science Department ('subsidiary', 0.8901183605194092) California\n",
            "1 ('subsidiary', 0.1874055713415146) Swanton Pacific\n",
            "1 ('main subject', 0.4886932969093323) 2\n",
            "1 ('subsidiary', 0.4866962432861328) Monterey pine forests\n",
            "1 ('subsidiary', 0.21422795951366425) California\n",
            "Swanton Pacific ('main subject', 0.7305606603622437) 2\n",
            "Swanton Pacific ('subsidiary', 0.9867450594902039) Monterey pine forests\n",
            "Swanton Pacific ('subsidiary', 0.9247258305549622) California\n",
            "2 ('subsidiary', 0.9254075288772583) Monterey pine forests\n",
            "2 ('subsidiary', 0.38923466205596924) California\n",
            "Monterey pine forests ('military branch', 0.3672320246696472) California\n",
            "ARI grant ('subsidiary', 0.9880728125572205) 1\n",
            "ARI grant ('main subject', 0.3340618312358856) 2\n",
            "ARI grant ('follows', 0.4026950001716614) Monterey pine pitch canker resistant trees\n",
            "1 ('developer', 0.245673269033432) 2\n",
            "1 ('main subject', 0.23493726551532745) Monterey pine pitch canker resistant trees\n",
            "2 ('developer', 0.767278254032135) Monterey pine pitch canker resistant trees\n",
            "3 ('follows', 0.6374277472496033) 0\n",
            "3 ('follows', 0.8381648063659668) 5\n",
            "3 ('follows', 0.36787065863609314) 6\n",
            "3 ('taxon rank', 0.9121668934822083) Monterey\n",
            "0 ('follows', 0.7050350308418274) 5\n",
            "0 ('instance of', 0.36671778559684753) 6\n",
            "0 ('taxon rank', 0.8839570879936218) Monterey\n",
            "5 ('manufacturer', 0.2943323850631714) 6\n",
            "5 ('taxon rank', 0.8621599078178406) Monterey\n",
            "6 ('taxon rank', 0.18047001957893372) Monterey\n",
            "Walter Sudden Oak Death Distribution, Detection, Ecological Impact ('field of work', 0.9456373453140259) Spread Modeling  \n",
            "recent years ('has part', 0.3223022520542145) Marin County\n",
            "recent years ('followed by', 0.7199748754501343) 10 surrounding counties\n",
            "Marin County ('taxon rank', 0.30454403162002563) 10 surrounding counties\n",
            "Marin County ('instance of', 0.48138657212257385) only last year\n",
            "Marin County ('taxon rank', 0.9315771460533142) Sudden Oak Death\n",
            "GIS ('developer', 0.7683963775634766) GIS model\n",
            "GIS ('main subject', 0.9972965121269226) SOD\n",
            "GIS model ('main subject', 0.9973737001419067) SOD\n",
            "SOD progression ('main subject', 0.4169760048389435) SOD\n",
            "the Scotts Creek Study Area ('follows', 0.6245855689048767) 2\n",
            "3 ('taxon rank', 0.9556898474693298) 4\n",
            "the beginning of July 2004 ('main subject', 0.4591529071331024) the USDA Forest Service\n",
            "the beginning of July 2004 ('has part', 0.7885459065437317) Institute of Forest Genetics\n",
            "the beginning of July 2004 ('follows', 0.7815536856651306) CRSIO\n",
            "the USDA Forest Service ('subsidiary', 0.9818680286407471) Institute of Forest Genetics\n",
            "the USDA Forest Service ('main subject', 0.8462581038475037) CRSIO\n",
            "Institute of Forest Genetics ('participant', 0.11945237219333649) CRSIO\n",
            "IFG ('has part', 0.6101272106170654) the California Polytechnic State University planting site\n",
            "IFG ('has part', 0.7028357982635498) Ano Nuevo\n",
            "IFG ('has part', 0.4996104836463928) 12 February 2005\n",
            "the California Polytechnic State University planting site ('has part', 0.5366489291191101) Ano Nuevo\n",
            "the California Polytechnic State University planting site ('follows', 0.4352315366268158) 12 February 2005\n",
            "Ano Nuevo ('followed by', 0.4984757900238037) 12 February 2005\n",
            "Swanton Pacific Ranch ('has part', 0.47727590799331665) Cal Poly SLO students\n",
            "FNR ('has part', 0.932256281375885) 475\n",
            "FNR ('follows', 0.2341918796300888) Sustainable Forestry and Environmental Practices\n",
            "475 ('main subject', 0.3981984853744507) Sustainable Forestry and Environmental Practices\n",
            "Swanton Pacific Ranch ('military branch', 0.5707311630249023) Interns\n",
            "year ('has part', 0.9922034740447998) Cal Poly SLO\n",
            "year ('has part', 0.37474524974823) Al Smith Day\n",
            "year ('has part', 0.9614948630332947) Swanton Pacific Ranch\n",
            "Cal Poly SLO ('participating team', 0.745931088924408) Al Smith Day\n",
            "Cal Poly SLO ('member of', 0.734453022480011) Swanton Pacific Ranch\n",
            "Al Smith Day ('member of', 0.8611502051353455) Swanton Pacific Ranch\n",
            "the Animal Science Department ('has part', 0.26651039719581604) Cal Poly SLO logging team\n",
            "Al Smith Day ('has part', 0.7012952566146851) Swanton Pacific Ranch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qhp5Ly9HZDhI"
      },
      "source": [
        "## Joint Entity and Relation Extraction: Partition Filter Network\n",
        "*** Uses a different version of transformers than OpenNRE***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8WcL1Hmx-AW",
        "outputId": "ea4caf1e-c050-44f1-c2be-d968e5cd0414"
      },
      "source": [
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E45T-EUOrsMr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea1282a5-a90f-47fe-d064-d8f2154d3e37"
      },
      "source": [
        "!git clone https://github.com/Coopercoppers/PFN.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PFN'...\n",
            "remote: Enumerating objects: 457, done.\u001b[K\n",
            "remote: Counting objects: 100% (408/408), done.\u001b[K\n",
            "remote: Compressing objects: 100% (396/396), done.\u001b[K\n",
            "remote: Total 457 (delta 233), reused 9 (delta 1), pack-reused 49\u001b[K\n",
            "Receiving objects: 100% (457/457), 9.70 MiB | 14.55 MiB/s, done.\n",
            "Resolving deltas: 100% (255/255), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RBmd01QZFS-",
        "outputId": "12f56ee8-1ca7-4a3d-e09d-622448f343bf"
      },
      "source": [
        "%cd ./PFN"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PFN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTUSDfmhZNhG",
        "outputId": "0c41fec0-6e28-49ea-ca41-f6b8b248397c"
      },
      "source": [
        "!pip install -r requirements.txt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.9.0\n",
            "  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 2.0 kB/s \n",
            "\u001b[?25hCollecting tqdm==4.51.0\n",
            "  Downloading tqdm-4.51.0-py2.py3-none-any.whl (70 kB)\n",
            "\u001b[K     |████████████████████████████████| 70 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting numpy==1.20.2\n",
            "  Downloading numpy-1.20.2-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3 MB 160 kB/s \n",
            "\u001b[?25hCollecting transformers==4.9.1\n",
            "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 28.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sentencepiece==0.1.96 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (0.1.96)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->-r requirements.txt (line 1)) (3.10.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.1->-r requirements.txt (line 4)) (2019.12.20)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.1->-r requirements.txt (line 4)) (0.0.46)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 24.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.1->-r requirements.txt (line 4)) (3.3.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.1->-r requirements.txt (line 4)) (21.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.1->-r requirements.txt (line 4)) (4.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.1->-r requirements.txt (line 4)) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 37.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.9.1->-r requirements.txt (line 4)) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.9.1->-r requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.1->-r requirements.txt (line 4)) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.1->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.1->-r requirements.txt (line 4)) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.1->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.9.1->-r requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.9.1->-r requirements.txt (line 4)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.9.1->-r requirements.txt (line 4)) (1.1.0)\n",
            "Installing collected packages: tqdm, tokenizers, pyyaml, numpy, huggingface-hub, transformers, torch\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.3\n",
            "    Uninstalling tqdm-4.62.3:\n",
            "      Successfully uninstalled tqdm-4.62.3\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.9.2\n",
            "    Uninstalling tokenizers-0.9.2:\n",
            "      Successfully uninstalled tokenizers-0.9.2\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 3.4.0\n",
            "    Uninstalling transformers-3.4.0:\n",
            "      Successfully uninstalled transformers-3.4.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.6.0\n",
            "    Uninstalling torch-1.6.0:\n",
            "      Successfully uninstalled torch-1.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed huggingface-hub-0.0.12 numpy-1.20.2 pyyaml-6.0 tokenizers-0.10.3 torch-1.9.0 tqdm-4.51.0 transformers-4.9.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "tokenizers",
                  "torch",
                  "tqdm",
                  "transformers"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fzEN261ByDfv",
        "outputId": "003f7645-8f76-4a19-ecef-8bad80017f01"
      },
      "source": [
        "sentences[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Swanton Pacific Ranch is a 3,200-acre ranch in Santa Cruz County, California, outside the town of Davenport.'"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHG2lrKEZOUV",
        "outputId": "48a20e6a-0f78-4ec1-f4c2-d5c686285bb3"
      },
      "source": [
        "!python inference.py \\\n",
        "--model_file /content/drive/MyDrive/CSC580-Model.SPR/Relation\\ Extraction/bert-nyt/nyt_test.pt \\\n",
        "--sent 'Swanton Pacific Ranch is a 3,200-acre (1,300ha) ranch in Santa Cruz County, California, outside the town of Davenport.'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 29.0/29.0 [00:00<00:00, 25.8kB/s]\n",
            "Downloading: 100% 570/570 [00:00<00:00, 467kB/s]\n",
            "Downloading: 100% 213k/213k [00:00<00:00, 2.70MB/s]\n",
            "Downloading: 100% 436k/436k [00:00<00:00, 3.38MB/s]\n",
            "Downloading: 100% 436M/436M [00:14<00:00, 29.2MB/s]\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Traceback (most recent call last):\n",
            "  File \"inference.py\", line 78, in <module>\n",
            "    model.load_state_dict(torch.load(args.model_file))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 607, in load\n",
            "    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 882, in _load\n",
            "    result = unpickler.load()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 857, in persistent_load\n",
            "    load_tensor(data_type, size, key, _maybe_decode_ascii(location))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 846, in load_tensor\n",
            "    loaded_storages[key] = restore_location(storage, location)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 175, in default_restore_location\n",
            "    result = fn(storage, location)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 151, in _cuda_deserialize\n",
            "    device = validate_cuda_device(location)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 135, in validate_cuda_device\n",
            "    raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
            "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6Y9cIFnaXPF",
        "outputId": "0a1c979f-d515-4859-f353-b32340119a8a"
      },
      "source": [
        "!python inference.py \\\n",
        "--model_file /content/drive/MyDrive/CSC580-Model.SPR/Relation\\ Extraction/bert-webnlg/web_test.pt \\\n",
        "--sent 'Swanton Pacific Ranch is a 3,200-acre (1,300ha) ranch in Santa Cruz County, California, outside the town of Davenport.'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Traceback (most recent call last):\n",
            "  File \"inference.py\", line 78, in <module>\n",
            "    model.load_state_dict(torch.load(args.model_file))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 607, in load\n",
            "    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 882, in _load\n",
            "    result = unpickler.load()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 857, in persistent_load\n",
            "    load_tensor(data_type, size, key, _maybe_decode_ascii(location))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 846, in load_tensor\n",
            "    loaded_storages[key] = restore_location(storage, location)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 175, in default_restore_location\n",
            "    result = fn(storage, location)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 151, in _cuda_deserialize\n",
            "    device = validate_cuda_device(location)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 135, in validate_cuda_device\n",
            "    raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
            "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbUNe1AafT_x",
        "outputId": "41f05578-173d-463d-c23c-298dd68a9795"
      },
      "source": [
        "!python inference.py \\\n",
        "--model_file /content/drive/MyDrive/CSC580-Model.SPR/Relation\\ Extraction/albert-ace2005/ace_test.pt \\\n",
        "--sent 'Headquartered in San Jose, California, Orchard Supply Hardware had dozens of locations throughout California, with expansions into Oregon and Florida.'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 760k/760k [00:00<00:00, 5.86MB/s]\n",
            "Downloading: 100% 1.31M/1.31M [00:00<00:00, 8.50MB/s]\n",
            "Downloading: 100% 706/706 [00:00<00:00, 551kB/s]\n",
            "Downloading: 100% 893M/893M [00:30<00:00, 29.3MB/s]\n",
            "Some weights of the model checkpoint at albert-xxlarge-v1 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
            "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Traceback (most recent call last):\n",
            "  File \"inference.py\", line 78, in <module>\n",
            "    model.load_state_dict(torch.load(args.model_file))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 607, in load\n",
            "    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 882, in _load\n",
            "    result = unpickler.load()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 857, in persistent_load\n",
            "    load_tensor(data_type, size, key, _maybe_decode_ascii(location))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 846, in load_tensor\n",
            "    loaded_storages[key] = restore_location(storage, location)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 175, in default_restore_location\n",
            "    result = fn(storage, location)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 151, in _cuda_deserialize\n",
            "    device = validate_cuda_device(location)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 135, in validate_cuda_device\n",
            "    raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
            "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWsypmpOf5pO",
        "outputId": "37b8e185-bbee-4952-9b3f-bd619853e970"
      },
      "source": [
        "!python inference.py \\\n",
        "--model_file /content/drive/MyDrive/CSC580-Model.SPR/Relation\\ Extraction/ACE2004/0/ace04_test_fold_0.pt \\\n",
        "--sent 'Headquartered in San Jose, California, Orchard Supply Hardware had dozens of locations throughout California, with expansions into Oregon and Florida.'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"inference.py\", line 77, in <module>\n",
            "    model = PFN(args, input_size, ner2idx, rel2idx)\n",
            "NameError: name 'input_size' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8sHzJCKkTHS",
        "outputId": "c55f7714-1d29-4efa-9f55-b04ddefa1cba"
      },
      "source": [
        "!python inference.py \\\n",
        "--model_file /content/drive/MyDrive/CSC580-Model.SPR/Relation\\ Extraction/scibert-scierc/sci_test.pt \\\n",
        "--sent 'Headquartered in San Jose, California, Orchard Supply Hardware had dozens of locations throughout California, with expansions into Oregon and Florida.'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 385/385 [00:00<00:00, 259kB/s]\n",
            "Downloading: 100% 228k/228k [00:00<00:00, 2.12MB/s]\n",
            "Downloading: 100% 442M/442M [00:12<00:00, 36.0MB/s]\n",
            "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Traceback (most recent call last):\n",
            "  File \"inference.py\", line 78, in <module>\n",
            "    model.load_state_dict(torch.load(args.model_file))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 607, in load\n",
            "    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 882, in _load\n",
            "    result = unpickler.load()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 857, in persistent_load\n",
            "    load_tensor(data_type, size, key, _maybe_decode_ascii(location))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 846, in load_tensor\n",
            "    loaded_storages[key] = restore_location(storage, location)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 175, in default_restore_location\n",
            "    result = fn(storage, location)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 151, in _cuda_deserialize\n",
            "    device = validate_cuda_device(location)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 135, in validate_cuda_device\n",
            "    raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
            "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrXF2m9Flvrd"
      },
      "source": [
        "import spacy\n",
        "\n",
        "doc = nlp('The ranch is owned and operated by California Polytechnic State University (Cal Poly) for educational and research in sustainable agriculture.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uKhq9goSELO"
      },
      "source": [
        "draw_dependency_graph(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6Q7YqQiSFBs",
        "outputId": "950ae318-d22a-4d62-a763-593afca411bd"
      },
      "source": [
        "doc = update_tokenizer(doc)\n",
        "rule_4(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The ranch',\n",
              "  'owned and operated by',\n",
              "  'California Polytechnic State University'),\n",
              " ('The ranch', 'owned and operated for', 'research'),\n",
              " ('The ranch', 'owned and operated in', 'sustainable agriculture')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "ga4JfUBtSnO2",
        "outputId": "9cccfc75-545d-49cc-f170-9f3eb1a16638"
      },
      "source": [
        "draw_dependency_graph(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"4a880250a99a48099e227b9fd526c77b-0\" class=\"displacy\" width=\"2325\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">ranch</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">owned and operated</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">by</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">California Polytechnic State University (</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Cal Poly)</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">for</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">educational</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">and</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">CCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">research</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">in</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">sustainable agriculture.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4a880250a99a48099e227b9fd526c77b-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4a880250a99a48099e227b9fd526c77b-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4a880250a99a48099e227b9fd526c77b-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,177.0 565.0,177.0 565.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4a880250a99a48099e227b9fd526c77b-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubjpass</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4a880250a99a48099e227b9fd526c77b-0-2\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4a880250a99a48099e227b9fd526c77b-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">auxpass</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4a880250a99a48099e227b9fd526c77b-0-3\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4a880250a99a48099e227b9fd526c77b-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">agent</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M735.0,354.0 L743.0,342.0 727.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4a880250a99a48099e227b9fd526c77b-0-4\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4a880250a99a48099e227b9fd526c77b-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M910.0,354.0 L918.0,342.0 902.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4a880250a99a48099e227b9fd526c77b-0-5\" stroke-width=\"2px\" d=\"M945,352.0 C945,264.5 1085.0,264.5 1085.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4a880250a99a48099e227b9fd526c77b-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1085.0,354.0 L1093.0,342.0 1077.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4a880250a99a48099e227b9fd526c77b-0-6\" stroke-width=\"2px\" d=\"M595,352.0 C595,89.5 1270.0,89.5 1270.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4a880250a99a48099e227b9fd526c77b-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1270.0,354.0 L1278.0,342.0 1262.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4a880250a99a48099e227b9fd526c77b-0-7\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4a880250a99a48099e227b9fd526c77b-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1435.0,354.0 L1443.0,342.0 1427.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4a880250a99a48099e227b9fd526c77b-0-8\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,264.5 1610.0,264.5 1610.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4a880250a99a48099e227b9fd526c77b-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1610.0,354.0 L1618.0,342.0 1602.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4a880250a99a48099e227b9fd526c77b-0-9\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,177.0 1790.0,177.0 1790.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4a880250a99a48099e227b9fd526c77b-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1790.0,354.0 L1798.0,342.0 1782.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4a880250a99a48099e227b9fd526c77b-0-10\" stroke-width=\"2px\" d=\"M595,352.0 C595,2.0 1975.0,2.0 1975.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4a880250a99a48099e227b9fd526c77b-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1975.0,354.0 L1983.0,342.0 1967.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4a880250a99a48099e227b9fd526c77b-0-11\" stroke-width=\"2px\" d=\"M1995,352.0 C1995,264.5 2135.0,264.5 2135.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4a880250a99a48099e227b9fd526c77b-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2135.0,354.0 L2143.0,342.0 2127.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zVVa7rlS0bG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}